{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "45d42c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
      "Hit:2 http://security.ubuntu.com/ubuntu jammy-security InRelease               \n",
      "Hit:3 http://archive.ubuntu.com/ubuntu jammy-updates InRelease                 \n",
      "Hit:4 http://archive.ubuntu.com/ubuntu jammy-backports InRelease               \n",
      "Hit:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
      "Hit:6 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
      "Reading package lists... Done\n",
      "^Cading package lists... 4%\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.3.3)\n",
      "Requirement already satisfied: snac in /usr/local/lib/python3.11/dist-packages (1.2.1)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.57.0)\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.8.0)\n",
      "Requirement already satisfied: datasets[audio] in /usr/local/lib/python3.11/dist-packages (4.1.1)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets[audio]) (3.16.1)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets[audio]) (21.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets[audio]) (0.4.0)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets[audio]) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets[audio]) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets[audio]) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets[audio]) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets[audio]) (2024.10.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets[audio]) (0.35.3)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets[audio]) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets[audio]) (6.0.2)\n",
      "Requirement already satisfied: torchcodec>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from datasets[audio]) (0.7.0)\n",
      "Requirement already satisfied: torch>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from datasets[audio]) (2.8.0)\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from snac) (0.8.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2025.9.18)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.8.0->datasets[audio]) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.8.0->datasets[audio]) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.8.0->datasets[audio]) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.8.0->datasets[audio]) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.11/dist-packages (from torch>=2.8.0->datasets[audio]) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.11/dist-packages (from torch>=2.8.0->datasets[audio]) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.11/dist-packages (from torch>=2.8.0->datasets[audio]) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.11/dist-packages (from torch>=2.8.0->datasets[audio]) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.8.0->datasets[audio]) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.11/dist-packages (from torch>=2.8.0->datasets[audio]) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.11/dist-packages (from torch>=2.8.0->datasets[audio]) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.11/dist-packages (from torch>=2.8.0->datasets[audio]) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.11/dist-packages (from torch>=2.8.0->datasets[audio]) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.8.0->datasets[audio]) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.8.0->datasets[audio]) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.11/dist-packages (from torch>=2.8.0->datasets[audio]) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.11/dist-packages (from torch>=2.8.0->datasets[audio]) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.8.0->datasets[audio]) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.8.0->datasets[audio]) (3.4.0)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.4.0->torch>=2.8.0->datasets[audio]) (77.0.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets[audio]) (3.13.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets[audio]) (1.1.10)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets[audio]) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets[audio]) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets[audio]) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets[audio]) (2025.1.31)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets[audio]) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets[audio]) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets[audio]) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets[audio]) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets[audio]) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets[audio]) (0.4.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets[audio]) (1.22.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch>=2.8.0->datasets[audio]) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.8.0->datasets[audio]) (2.1.5)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "!apt-get update\n",
    "!apt-get install -y ffmpeg\n",
    "%pip install pandas datasets[audio] snac transformers torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cfdef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "def get_dataset_shard():\n",
    "    parser = argparse.ArgumentParser(description=\"Process audio dataset shard\")\n",
    "    parser.add_argument(\n",
    "        \"--dataset-shard\",\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help=\"Dataset shard name (e.g., dataset-000000)\",\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "    return args.dataset_shard\n",
    "\n",
    "\n",
    "dataset_shard = get_dataset_shard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb09bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import locale\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
    "import torch\n",
    "from snac import SNAC\n",
    "\n",
    "\n",
    "model = SNAC.from_pretrained(\"hubertsiuzdak/snac_24khz\")\n",
    "model.to(\"cuda\")\n",
    "\n",
    "\n",
    "import torchaudio.transforms as T\n",
    "from datasets import Audio\n",
    "\n",
    "def tokenise_audio(waveform, ):\n",
    "  waveform = torch.from_numpy(waveform).unsqueeze(0)\n",
    "  waveform = waveform.to(dtype=torch.float32)\n",
    "  # resample_transform = T.Resample(orig_freq=24000, new_freq=24000)\n",
    "  # waveform = resample_transform(waveform)\n",
    "  waveform = waveform.unsqueeze(0).to(\"cuda\")\n",
    "  #generate the codes from snac\n",
    "  with torch.inference_mode():\n",
    "    codes = model.encode(waveform)\n",
    "\n",
    "  all_codes = []\n",
    "  for i in range(codes[0].shape[1]):\n",
    "    all_codes.append(codes[0][0][i].item()+128266)\n",
    "    all_codes.append(codes[1][0][2*i].item()+128266+4096)\n",
    "    all_codes.append(codes[2][0][4*i].item()+128266+(2*4096))\n",
    "    all_codes.append(codes[2][0][(4*i)+1].item()+128266+(3*4096))\n",
    "    all_codes.append(codes[1][0][(2*i)+1].item()+128266+(4*4096))\n",
    "    all_codes.append(codes[2][0][(4*i)+2].item()+128266+(5*4096))\n",
    "    all_codes.append(codes[2][0][(4*i)+3].item()+128266+(6*4096))\n",
    "\n",
    "\n",
    "  return all_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b2f510",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:00, 17.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          codes_list  \\\n",
      "0  [132319, 133914, 136569, 143287, 144830, 15172...   \n",
      "1  [129190, 135722, 140176, 140864, 145752, 15190...   \n",
      "2  [130910, 133766, 138340, 143914, 145619, 15084...   \n",
      "3  [129639, 134513, 138212, 142043, 147157, 15085...   \n",
      "4  [128689, 136166, 138281, 144531, 148160, 15257...   \n",
      "5  [131287, 133988, 137566, 141145, 146728, 15033...   \n",
      "\n",
      "                                                 txt  \n",
      "0  Att Sverige spelar semifinal i VM för U17 lag ...  \n",
      "1  I helgens tävling på 15 kilometer, fristil, bl...  \n",
      "2  Bara en svensk löpare har sprungit snabbare än...  \n",
      "3  Regeringen ska undersöka om lagen om våldtäkt ...  \n",
      "4  Lothar Schalin vann diamantbollen, priset till...  \n",
      "5                    Utredningen är på 10 000 sidor.  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "df = load_dataset(\"cubbk/audio_swedish_2_dataset_cleaned\", split='train', streaming=True, data_dir=\"8sidor_text_speech_dataset\", data_files=f\"{dataset_shard}.tar\")\n",
    "\n",
    "tokenised_df_arr = []\n",
    "\n",
    "for i, dataset_item in tqdm(enumerate(df)):\n",
    "    if(i > 5):\n",
    "        break\n",
    "    \n",
    "    wf = np.asarray(dataset_item[\"wav\"][\"array\"], dtype=np.float32)\n",
    "    codes_list = tokenise_audio(wf)\n",
    "    tokenised_df_arr.append({\n",
    "        \"codes_list\": codes_list,\n",
    "        \"txt\": dataset_item[\"txt\"]\n",
    "    })\n",
    "\n",
    "tokenized_df = pd.DataFrame(tokenised_df_arr)\n",
    "print(tokenized_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42efcd4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_proc must be <= 6. Reducing num_proc to 6 for dataset of size 6.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ba6bba955e54b6c8011d8f74fc5a00b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=6):   0%|          | 0/6 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_proc must be <= 6. Reducing num_proc to 6 for dataset of size 6.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** HERE you can modify the text prompt\n",
      "i.e. if you wanted a multispeaker model like canopylabs/orpheus-3b-0.1-ft, you can pass:\n",
      "f\"{example[\"source\"]}:  {example[\"text\"]}\", as is passed.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9380a789dd9b48fb900bcd033796ff61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=6):   0%|          | 0/6 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text_tokens', 'input_ids', 'labels', 'attention_mask'],\n",
       "    num_rows: 6\n",
       "})"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import os\n",
    "\n",
    "tokeniser_length = 128256\n",
    "start_of_text = 128000\n",
    "end_of_text = 128009\n",
    "\n",
    "start_of_speech = tokeniser_length + 1\n",
    "end_of_speech = tokeniser_length + 2\n",
    "\n",
    "start_of_human = tokeniser_length + 3\n",
    "end_of_human = tokeniser_length + 4\n",
    "\n",
    "start_of_ai = tokeniser_length + 5\n",
    "end_of_ai =  tokeniser_length + 6\n",
    "pad_token = tokeniser_length + 7\n",
    "\n",
    "audio_tokens_start = tokeniser_length + 10\n",
    "\n",
    "tokenizer_name = \"canopylabs/orpheus-3b-0.1-pretrained\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "num_proc = os.cpu_count() - 2\n",
    "\n",
    "# Fix: Use boolean indexing instead of .filter() for pandas DataFrame\n",
    "ds = tokenized_df[tokenized_df[\"codes_list\"].notna()]\n",
    "ds = ds[ds[\"codes_list\"].apply(lambda x: len(x) > 0)]\n",
    "\n",
    "# Convert to datasets.Dataset for the map operations\n",
    "from datasets import Dataset\n",
    "ds = Dataset.from_pandas(ds)\n",
    "\n",
    "#@title Create Input Ids\n",
    "def remove_duplicate_frames(example):\n",
    "    vals = example[\"codes_list\"]\n",
    "    if len(vals) % 7 != 0:\n",
    "        raise ValueError(\"Input list length must be divisible by 7\")\n",
    "\n",
    "    result = vals[:7]\n",
    "\n",
    "    removed_frames = 0\n",
    "\n",
    "    for i in range(7, len(vals), 7):\n",
    "        current_first = vals[i]\n",
    "        previous_first = result[-7]\n",
    "\n",
    "        if current_first != previous_first:\n",
    "            result.extend(vals[i:i+7])\n",
    "        else:\n",
    "            removed_frames += 1\n",
    "\n",
    "    example[\"codes_list\"] = result\n",
    "\n",
    "    return example\n",
    "\n",
    "ds = ds.map(remove_duplicate_frames, num_proc=num_proc)\n",
    "\n",
    "tok_info = '''*** HERE you can modify the text prompt\n",
    "i.e. if you wanted a multispeaker model like canopylabs/orpheus-3b-0.1-ft, you can pass:\n",
    "f\"{example[\"source\"]}:  {example[\"text\"]}\", as is passed.\n",
    "'''\n",
    "print(tok_info)\n",
    "\n",
    "def create_input_ids(example):\n",
    "    text_ids = tokenizer.encode(example[\"txt\"],  add_special_tokens=True)\n",
    "    text_ids.append(end_of_text)\n",
    "    example[\"text_tokens\"] = text_ids\n",
    "    input_ids = (\n",
    "        [start_of_human]\n",
    "        + example[\"text_tokens\"]\n",
    "        + [end_of_human]\n",
    "        + [start_of_ai]\n",
    "        + [start_of_speech]\n",
    "        + example[\"codes_list\"]\n",
    "        + [end_of_speech]\n",
    "        + [end_of_ai]\n",
    "    )\n",
    "    example[\"input_ids\"] = input_ids\n",
    "    example[\"labels\"] = input_ids\n",
    "    example[\"attention_mask\"] = [1] * len(input_ids)\n",
    "\n",
    "    return example\n",
    "\n",
    "ds = ds.map(create_input_ids, num_proc=num_proc, remove_columns=[\"txt\", \"codes_list\"])\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abdeece",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = [\"input_ids\", \"labels\", \"attention_mask\"]\n",
    "columns_to_remove = [col for col in ds.column_names if col not in columns_to_keep]\n",
    "\n",
    "ds = ds.remove_columns(columns_to_remove)\n",
    "\n",
    "ds\n",
    "\n",
    "# ds.push_to_hub(\n",
    "#     \"cubbk/audio_swedish_2_dataset_cleaned\",  # Your existing dataset repo\n",
    "#     config_name=\"8sidor_tokenized\",  # Optional: create a separate config\n",
    "#     split=\"train\",\n",
    "#     commit_message=\"Add tokenized parquet file\",\n",
    "#     create_pr=True  # Set to True if you want to create a pull request instead\n",
    "# )\n",
    "\n",
    "df_to_save = ds.to_pandas()\n",
    "df_to_save.to_parquet(f\"{dataset_shard}_tokenized.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd884dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_gcs():\n",
    "    os.system(\n",
    "        f\"gsutil cp {dataset_shard}_tokenized.parquet gs://audio_swedish_2/tokenized/{dataset_shard}_tokenized.parquet\"\n",
    "    )\n",
    "    \n",
    "upload_to_gcs()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
